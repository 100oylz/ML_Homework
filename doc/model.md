## 第一章：基础循环神经网络模型 (Simple RNN)

### 1.1 模型结构

- **核心层**：`nn.RNN`。采用 `batch_first=True` 配置，使输入张量格式固定为 `(batch, seq_len, input_size)`。
- **深度配置**：支持通过 `num_layers` 参数进行多层堆叠。当层数大于 1 时，自动启用层间 `dropout`。
- **正则化**：在全连接层（Linear Layer）之前额外集成了一个 `nn.Dropout` 层，以进一步防止过拟合。
- **输出层**：`nn.Linear`，负责将隐藏层特征映射至最终的输出维度。

### 1.2 运算逻辑

1. **特征提取**：模型接收完整序列，经过 RNN 层计算后，提取**最后一个时间步**的隐藏状态 `rnn_out[:, -1, :]`。
2. **预测输出**：将提取的末端特征输入 Dropout 和线性层，得到最终预测结果。
3. **多值返回**：前向传播同时返回预测值 `out`、完整序列输出 `rnn_out` 以及隐藏状态 `hidden`。

------

## 第二章：门控循环单元模型 (GRU)

### 2.1 模型结构

- **核心层**：`nn.GRU`。相比于标准 RNN，GRU 通过重置门（Reset Gate）和更新门（Update Gate）有效缓解了长序列训练中的梯度消失问题，且参数量较少。
- **参数配置**：
  - `hidden_size`: 决定了门控单元的内部维度。
  - `dropout`: 默认为 `0.3`，在多层结构中对非末层输出进行随机失活。

### 2.2 运算逻辑

1. **门控计算**：通过内部机制自适应地控制历史信息流向当前状态。
2. **映射流程**：提取 GRU 最后一层的末位时间步特征 `gru_out[:, -1, :]`。
3. **结果输出**：经过输出 Dropout 后进行线性变换，返回预测值及完整的状态信息。

------

## 第三章：长短期记忆网络模型 (LSTM)

### 3.1 模型结构

- **核心层**：`nn.LSTM`。利用遗忘门、输入门和输出门构成的复杂单元，能够捕捉极长距离的时间依赖关系。
- **双状态机制**：除了隐藏状态（Hidden State），LSTM 额外维护一个细胞状态（Cell State），用于稳定传递长期记忆。

### 3.2 运算逻辑

1. **记忆流转**：输入序列驱动单元内部的三个门控开关，决定 $c_t$（细胞状态）中信息的保留与更新。
2. **序列聚合**：采用与前两类模型一致的策略，提取序列最后一帧的输出 `lstm_out[:, -1, :]` 作为代表性特征。
3. **状态返回**：前向传播结束后，返回预测结果以及包含 `(h_n, c_n)` 的状态元组，满足更复杂的序列推理需求。

## 第四章：时间序列 Transformer 模型 (TimeSeries Transformer)

### 4.1 模型架构：Encoder-only 结构

本模型采用了基于 **Transformer Encoder** 的纯编码器架构，旨在通过自注意力机制（Self-Attention）捕捉序列内部的全局依赖关系。

- **输入投影层 (Input Projection)**：
  - **实现**：`nn.Linear(input_size, hidden_size)`。
  - **作用**：将原始输入特征维度 $F$ 线性映射到 Transformer 的内部隐藏维度 $d_{model}$（即 `hidden_size`），为后续的注意力计算对齐维度。
- **编码器核心 (Transformer Encoder)**：
  - **组件**：由多个 `nn.TransformerEncoderLayer` 堆叠而成（通过 `num_layers` 指定）。
  - **参数配置**：
    - `nhead`: 多头注意力机制（Multi-head Attention）的数量，用于并行捕捉不同子空间的特征。
    - `dropout`: 内部集成了 Dropout 以增强模型稳健性。
    - `batch_first=True`: 确保输入数据格式兼容 `(Batch, Sequence, Features)`。
- **输出预测头 (Output Head)**：
  - **实现**：`nn.Linear(hidden_size, output_size)`。
  - **作用**：将经过编码器增强的特征表示映射回目标预测维度。

### 4.2 运算逻辑

1. **特征映射**：输入张量 `x` 首先经过 `input_proj` 进行升维或降维，转换为 Transformer 可处理的隐藏向量。
2. **上下文编码**：数据进入 `transformer_encoder`。通过自注意力机制，序列中的每个时间步都会根据全序列的信息更新其表达。
3. **全局特征提取**：采用**均值池化 (Mean Pooling)** 策略，通过 `x.mean(dim=1)` 计算序列在时间维度上的平均值，从而获得能够代表整段序列的全局特征向量。
4. **预测生成**：最终通过线性层 `fc_out` 输出预测值。